---
title: "Telco Customer Churn: End-to-End Analysis and Modeling"
author: "Generated by Cascade"
date: "`r format(Sys.time(), '%Y-%m-%d %H:%M')`"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: true
    df_print: paged
  pdf_document: default
  word_document: default
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = TRUE, warning = TRUE, fig.width = 8, fig.height = 5)
```

# Overview

- **Goal**: Clean and explore the Telco churn dataset, then build and evaluate predictive models (Logistic Regression, Random Forest) to predict customer churn.
- **Deliverables**: EDA visuals, model training, evaluation metrics (accuracy, sensitivity, specificity), and guidance for next steps.

# Environment and Packages

```{r libraries}
# Data wrangling and plotting
library(tidyverse)
library(ggplot2)

# Modeling utilities
library(caret)        # data partitioning + confusionMatrix
library(randomForest) # Random Forest classifier

# Extended analysis
library(pROC)   # ROC/AUC
library(broom)  # tidy model summaries and odds ratios

# If you need to read an Excel file, uncomment the line below
# library(readxl)
```

# Data Loading

- The script assumes the file is in the working directory and can be read via `read.csv`.
- If your file is an Excel `.xls`, use `readxl::read_excel()` instead (shown below commented out).

```{r load-data}
# Update the path if your file is in a different location
DATA_PATH <- "WA_Fn-UseC_-Telco-Customer-Churn.xls"

# CSV-like loading (as currently stored)
churn_data <- read.csv(DATA_PATH)

# If the file is truly Excel (.xls), use this instead:
# churn_data <- readxl::read_excel(DATA_PATH)

# Quick peek
str(churn_data)
```

# Data Cleaning and Preparation

- Convert `TotalCharges` to numeric and impute NAs with 0.
- Remove `customerID` (identifier, not predictive).
- Recode service columns to reduce sparse factor levels:
  - Replace "No internet service" with "No" for: `OnlineSecurity`, `OnlineBackup`, `DeviceProtection`, `TechSupport`, `StreamingTV`, `StreamingMovies`.
  - Replace "No phone service" with "No" in `MultipleLines`.
- Convert remaining character columns to factor for modeling.

```{r cleaning}
churn_data <- churn_data %>%
  mutate(TotalCharges = as.numeric(TotalCharges)) %>%
  mutate(TotalCharges = ifelse(is.na(TotalCharges), 0, TotalCharges)) %>%
  select(-customerID)

churn_data <- churn_data %>%
  mutate(across(c(OnlineSecurity, OnlineBackup, DeviceProtection, TechSupport,
                  StreamingTV, StreamingMovies),
                ~ recode(., "No internet service" = "No"))) %>%
  mutate(MultipleLines = recode(MultipleLines, "No phone service" = "No"))

churn_data <- churn_data %>%
  mutate(across(where(is.character), as.factor))

# Confirm target is factor
churn_data$Churn <- as.factor(churn_data$Churn)
```

# Exploratory Data Analysis (EDA)

## Churn Rate
```{r churn-rate}
churn_rate <- churn_data %>%
  count(Churn) %>%
  mutate(Proportion = n / sum(n))
churn_rate
```

## Churn by Contract Type
```{r plot-contract}
churn_data %>%
  ggplot(aes(x = Contract, fill = Churn)) +
  geom_bar(position = "dodge") +
  labs(title = "Customer Churn by Contract Type",
       x = "Contract Type",
       y = "Number of Customers") +
  theme_minimal()
```

## Churn by Internet Service
```{r plot-internet}
churn_data %>%
  ggplot(aes(x = InternetService, fill = Churn)) +
  geom_bar(position = "dodge") +
  labs(title = "Customer Churn by Internet Service Type",
       x = "Internet Service",
       y = "Number of Customers") +
  theme_minimal()
```

## Monthly Charges by Churn
```{r plot-monthly}
churn_data %>%
  ggplot(aes(x = Churn, y = MonthlyCharges, fill = Churn)) +
  geom_boxplot() +
  labs(title = "Monthly Charges by Churn Status",
       x = "Churn Status",
       y = "Monthly Charges ($)") +
  theme_minimal()
```

## Correlation Among Numeric Features
```{r corr}
numeric_data <- churn_data %>% select(tenure, MonthlyCharges, TotalCharges)
correlation_matrix <- cor(numeric_data)
correlation_matrix
```

# Train/Test Split

- Use an 80/20 split with `caret::createDataPartition()` to preserve class proportions.
- Set a seed to ensure reproducibility.

```{r split}
set.seed(123)
trainIndex <- createDataPartition(churn_data$Churn, p = 0.8, list = FALSE, times = 1)
train_data <- churn_data[trainIndex, ]
test_data  <- churn_data[-trainIndex, ]

list(train_n = nrow(train_data), test_n = nrow(test_data))
```

# Modeling

## Logistic Regression (binomial GLM)

- Interpretable coefficients indicating direction/strength of effects on churn odds.

```{r model-logistic}
log_model <- glm(Churn ~ ., data = train_data, family = binomial)
summary(log_model)
```

## Random Forest

- Captures non-linearities and interactions; robust baseline accuracy.

```{r model-rf}
rf_model <- randomForest(Churn ~ ., data = train_data)
rf_model
```

# Evaluation

- Make predictions on the held-out test set.
- For logistic regression, convert probabilities to classes using 0.5 threshold.
- Compute confusion matrices for both models with the positive class set to "Yes".

```{r eval}
# Logistic Regression predictions
log_predictions <- predict(log_model, test_data, type = "response")
log_pred_class <- factor(ifelse(log_predictions > 0.5, "Yes", "No"),
                         levels = levels(test_data$Churn))
cm_log <- confusionMatrix(log_pred_class, test_data$Churn, positive = "Yes")
cm_log

# Random Forest predictions
rf_predictions <- predict(rf_model, test_data)
rf_predictions <- factor(rf_predictions, levels = levels(test_data$Churn))
cm_rf <- confusionMatrix(rf_predictions, test_data$Churn, positive = "Yes")
cm_rf
```

## Logistic Regression: Odds Ratios and Interpretation

- We exponentiate coefficients to obtain Odds Ratios (OR). OR > 1 increases churn odds; OR < 1 decreases.

```{r log-odds-ratios}
# Tidy coefficient table and compute Odds Ratios with 95% CI
log_tidy <- broom::tidy(log_model, conf.int = TRUE) %>%
  mutate(odds_ratio = exp(estimate),
         or_low = exp(conf.low),
         or_high = exp(conf.high)) %>%
  arrange(desc(abs(estimate)))
log_tidy %>% select(term, estimate, odds_ratio, or_low, or_high, p.value) %>% head(20)
```

## ROC Curves and AUC

- ROC shows the trade-off between sensitivity and specificity across thresholds. AUC closer to 1 indicates better separability.

```{r roc-auc, fig.width=7, fig.height=5}
# Logistic Regression ROC/AUC
roc_log <- pROC::roc(response = test_data$Churn,
                     predictor = as.numeric(log_predictions),
                     levels = c("No","Yes"))
auc_log <- pROC::auc(roc_log)

# Random Forest ROC/AUC (use class probabilities)
rf_prob <- predict(rf_model, test_data, type = "prob")[, "Yes"]
roc_rf <- pROC::roc(response = test_data$Churn,
                    predictor = rf_prob,
                    levels = c("No","Yes"))
auc_rf <- pROC::auc(roc_rf)

par(mar = c(4,4,1,1))
plot(roc_log, col = "steelblue", lwd = 2, main = sprintf("ROC Curves (Log AUC=%.3f, RF AUC=%.3f)", as.numeric(auc_log), as.numeric(auc_rf)))
plot(roc_rf, col = "darkorange", lwd = 2, add = TRUE)
legend("bottomright", legend = c(paste0("Logistic (AUC=", round(as.numeric(auc_log),3), ")"),
                                 paste0("Random Forest (AUC=", round(as.numeric(auc_rf),3), ")")),
       col = c("steelblue","darkorange"), lwd = 2, bty = "n")
```

## Variable Importance

- Random Forest importance via `caret::varImp`.
- Logistic importance approximated by absolute coefficient magnitude (note: scale-dependent, interpret carefully).

```{r var-importance}
# RF variable importance
rf_imp <- caret::varImp(rf_model)
rf_imp

# Top features plot for RF
rf_imp_df <- rf_imp$importance %>%
  rownames_to_column("feature") %>%
  arrange(desc(Overall)) %>%
  slice(1:20)
ggplot(rf_imp_df, aes(x = reorder(feature, Overall), y = Overall)) +
  geom_col(fill = "#2c7fb8") +
  coord_flip() +
  labs(title = "Random Forest: Top 20 Feature Importances", x = "Feature", y = "Importance (caret::varImp)") +
  theme_minimal()

# Logistic: coefficient magnitude (for reference)
log_coef <- broom::tidy(log_model) %>% filter(term != "(Intercept)") %>%
  mutate(abs_coef = abs(estimate)) %>%
  arrange(desc(abs_coef)) %>% slice(1:20)
ggplot(log_coef, aes(x = reorder(term, abs_coef), y = abs_coef)) +
  geom_col(fill = "#f03b20") +
  coord_flip() +
  labs(title = "Logistic Regression: Top 20 |Coefficient|", x = "Term", y = "|Coefficient|") +
  theme_minimal()
```

## Linear Probability Model (LPM)

- Fits a Linear Regression to a binary target (1 for Yes churn, 0 for No). Simple and sometimes informative, but has limitations:
  - Predicted values can fall outside [0,1].
  - Assumes constant variance and linearity, which may not hold.

```{r lpm}
# Create numeric target for LPM
train_data <- train_data %>% mutate(Churn_num = if_else(Churn == "Yes", 1, 0))
test_data  <- test_data  %>% mutate(Churn_num = if_else(Churn == "Yes", 1, 0))

# Fit linear regression
lpm_model <- lm(Churn_num ~ . - Churn, data = train_data)
summary(lpm_model)

# Predict numeric probabilities (may be outside [0,1])
lpm_pred_num <- predict(lpm_model, newdata = test_data)

# Evaluate RMSE and bounded rate
rmse <- sqrt(mean((lpm_pred_num - test_data$Churn_num)^2))
prop_out_of_bounds <- mean(lpm_pred_num < 0 | lpm_pred_num > 1)
list(RMSE = rmse, Proportion_Out_Of_[0,1] = prop_out_of_bounds)

# Classify at 0.5 and evaluate
lpm_pred_class <- factor(ifelse(lpm_pred_num > 0.5, "Yes", "No"), levels = levels(test_data$Churn))
cm_lpm <- confusionMatrix(lpm_pred_class, test_data$Churn, positive = "Yes")
cm_lpm
```

# Interpretation and Next Steps

- Accuracy around ~80% is typical for this dataset; sensitivity is lower due to class imbalance.
- Depending on business goals, you might:
  - Increase recall of churners by lowering the threshold from 0.5 and selecting by ROC/PR metrics.
  - Use cross-validation with `caret::train()` to tune RF (e.g., `mtry`) and consider regularized logistic (`glmnet`).
  - Review feature importance (`varImp`) and model coefficients to understand key drivers.

```{r session-info, echo=FALSE}
sessionInfo()
```

# Reproducibility: How to Knit

- Open this file (`report.Rmd`) in RStudio.
- Install dependencies once:

```r
install.packages(c("tidyverse","ggplot2","caret","randomForest","e1071","pROC","broom"))
# If using Excel input:
# install.packages("readxl")
```

- Ensure the dataset file `WA_Fn-UseC_-Telco-Customer-Churn.xls` is in the same folder as this report.
- Click Knit (choose HTML/PDF/Word) to generate the report.
- Alternatively, from the Console:

```r
rmarkdown::render("report.Rmd", output_format = "html_document")
```
